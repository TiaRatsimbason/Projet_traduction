{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "154ebd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a2eca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2023e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Phrases\n",
      "0  new jersey est parfois calme pendant l' automn...\n",
      "1  les états-unis est généralement froid en juill...\n",
      "2  california est généralement calme en mars , et...\n",
      "3  les états-unis est parfois légère en juin , et...\n",
      "4  votre moins aimé fruit est le raisin , mais mo...\n",
      "                                             Phrases\n",
      "0  new jersey is sometimes quiet during autumn , ...\n",
      "1  the united states is usually chilly during jul...\n",
      "2  california is usually quiet during march , and...\n",
      "3  the united states is sometimes mild during jun...\n",
      "4  your least liked fruit is the grape , but my l...\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "\n",
    "# chemin et nom du fichier \n",
    "fichier_fr = \"small_vocab_fr\"\n",
    "\n",
    "# Lire les lignes du fichier texte dans une liste\n",
    "with open(fichier_fr, \"r\") as fichier:\n",
    "    lignes = fichier.readlines()\n",
    "    \n",
    "# Créer un DataFrame à partir de la liste de lignes\n",
    "df_fr = pd.DataFrame({\"Phrases\": lignes})\n",
    "\n",
    "\n",
    "# même méthode pour le df English\n",
    "# chemin et nom du fichier \n",
    "fichier_en = \"small_vocab_en\"\n",
    "\n",
    "# Lire les lignes du fichier texte dans une liste\n",
    "with open(fichier_en, \"r\") as fichier:\n",
    "    lignes = fichier.readlines()\n",
    "    \n",
    "# Créer un DataFrame à partir de la liste de lignes\n",
    "df_en = pd.DataFrame({\"Phrases\": lignes})\n",
    "\n",
    "# Afficher les premières lignes des DataFrame pour vérifier\n",
    "print(df_fr.head())\n",
    "print(df_en.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "082a1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BAG OF WORDS ####\n",
    "\n",
    "# import\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Création d'une fonction pour créer le bag of words d'un dataframe\n",
    "def BOW (dataframe):\n",
    "    \n",
    "    # Créez une instance CountVectorizer\n",
    "    vectorizer = CountVectorizer( ) # paramètre par défaut car résulat bof avec count_vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), token_pattern=r\"[^' ']+\" )\n",
    "\n",
    "    # Vectorizer le df ( crééer le bag of words)  soit calcul du nombre d'apparition de chaque mot dans la phrase de la colonne \"Phrase\"\n",
    "    bow = vectorizer.fit_transform(dataframe[\"Phrases\"])\n",
    "\n",
    "    # Converti la matrice numpy en sortie du vectorize en df\n",
    "    bow_df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Renvoie le bow au format df en sortie de la fonction, et également le vectorizer car on va le réutiliser pour .transform lorsqu'on voudra 'bowiser' une nouvelle phrase d'apèrs le bow existant \n",
    "    return bow_df, vectorizer\n",
    "\n",
    "\n",
    "# Création du bow du dataframe français et anglais\n",
    "bow_fr, vectorizer_fr = BOW(df_fr)\n",
    "bow_en, vectorizer_en = BOW(df_en)\n",
    "\n",
    "# Print (bow_fr.head(),bow_en.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d1f7a921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>animal</th>\n",
       "      <th>animals</th>\n",
       "      <th>apple</th>\n",
       "      <th>apples</th>\n",
       "      <th>april</th>\n",
       "      <th>are</th>\n",
       "      <th>aren</th>\n",
       "      <th>august</th>\n",
       "      <th>...</th>\n",
       "      <th>where</th>\n",
       "      <th>white</th>\n",
       "      <th>why</th>\n",
       "      <th>winter</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>would</th>\n",
       "      <th>yellow</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>langue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   am  and  animal  animals  apple  apples  april  are  aren  august  ...  \\\n",
       "0   0    1       0        0      0       0      1    0     0       0  ...   \n",
       "1   0    1       0        0      0       0      0    0     0       0  ...   \n",
       "2   0    1       0        0      0       0      0    0     0       0  ...   \n",
       "3   0    1       0        0      0       0      0    0     0       0  ...   \n",
       "4   0    0       0        0      1       0      0    0     0       0  ...   \n",
       "\n",
       "   where  white  why  winter  wonderful  would  yellow  you  your  langue  \n",
       "0      0      0    0       0          0      0       0    0     0      en  \n",
       "1      0      0    0       0          0      0       0    0     0      en  \n",
       "2      0      0    0       0          0      0       0    0     0      en  \n",
       "3      0      0    0       0          0      0       0    0     0      en  \n",
       "4      0      0    0       0          0      0       0    0     1      en  \n",
       "\n",
       "[5 rows x 197 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajout d'une colonne langue et du tag en ou fr correspondant\n",
    "bow_fr['langue']='fr'\n",
    "bow_en['langue']='en'\n",
    "\n",
    "# Afficher les premières lignes des DataFrame pour vérifier\n",
    "bow_fr.head()\n",
    "bow_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "86e52326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Concatenation des bow en un seul dataset\n",
    "data = pd.concat([bow_fr, bow_en], axis=0)\n",
    "\n",
    "# Remplacement des Nan (générés suite à la concaténation) en zéro\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Comptez le nombre de NaN dans chaque colonne, puis sum supplémentaire pour avoir le nomre total de Nan\n",
    "nan_count = data.isna().sum().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3e752da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des jeux target et feature\n",
    "X = data.drop(columns=[\"langue\"])\n",
    "y = data[\"langue\"]\n",
    "\n",
    "# Division des données en ensembles d'apprentissage et de test (80% pour l'apprentissage, 20% pour le test et un prélèvement random)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "72f18810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle k-NN : 1.0\n"
     ]
    }
   ],
   "source": [
    "### Entrainement d'un premier modèle KNN\n",
    "\n",
    "# Import\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Créer un objet k-NN (k-Nearest Neighbors)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # valeur à ajuster pê\n",
    "\n",
    "# Entraîner le modèle k-NN sur les données d'entraînement\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculer la précision du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Afficher la précision\n",
    "print(\"Précision du modèle k-NN :\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc7d66d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new jersey est parfois calme pendant l' automn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>les états-unis est généralement froid en juill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california est généralement calme en mars , et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>les états-unis est parfois légère en juin , et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>votre moins aimé fruit est le raisin , mais mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Phrases\n",
       "0  new jersey est parfois calme pendant l' automn...\n",
       "1  les états-unis est généralement froid en juill...\n",
       "2  california est généralement calme en mars , et...\n",
       "3  les états-unis est parfois légère en juin , et...\n",
       "4  votre moins aimé fruit est le raisin , mais mo..."
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Générateur de phrase à détecter étape 1\n",
    "\n",
    "# Constituer un réservoir de phrases utilisant les mots du bow, ce qui revient à concatener les df_fr et df_en (et pas les bow_fr et bow_en)\n",
    "phrases_tank = pd.concat([df_fr, df_en], axis=0)\n",
    "phrases_tank.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "27d68aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La phrase a détecter est :\n",
      " son fruit préféré est la mangue , mais notre préféré est la poire .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Générateur de phrase à détecter étape 2\n",
    "\n",
    "# Prendre une phrase au hasard utilisant les mots du bow\n",
    "phrase_random = phrases_tank['Phrases'].sample(n=1).iloc[0]\n",
    "print(\"La phrase a détecter est :\\n\", phrase_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "89f23da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Générateur de phrase à détecter étape 3 (optionnel)\n",
    "\n",
    "# l'idée est de prendre X mots au hasard dans le bow commun\n",
    "# même si aucun sens ou pas dans la même langue, juste pour voir quelle langue est atrribuée lors de la détection\n",
    "\n",
    "import random\n",
    "\n",
    "# Obtenir le vocabulaire commun à partir du vectorizer commun\n",
    "bow_commun = vectorizer_commun.get_feature_names_out()\n",
    "\n",
    "# Définir le nombre de mots que vous souhaitez dans la phrase au hasard (X)\n",
    "X = 5  # Changez X selon les besoins\n",
    "\n",
    "# Sélectionnez X mots au hasard à partir du vocabulaire commun\n",
    "mots_au_hasard = random.sample(vocabulaire_commun, X)\n",
    "\n",
    "# Construisez une phrase au hasard en utilisant les mots sélectionnés\n",
    "phrase_au_hasard = \" \".join(mots_au_hasard)\n",
    "\n",
    "# Affichez la phrase au hasard\n",
    "print(\"Phrase au hasard :\", phrase_au_hasard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f3f08a79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m phrase_random_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhrases\u001b[39m\u001b[38;5;124m'\u001b[39m: phrase_random},index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# transfo de la phrase 'string' en dataframe car ma fonction BOW fonctionne avec des dataframes\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#bow_phrase = BOW(phrase_random_df) ## hé non ! car cela créer un nouveau bow alors qu'il faut analyser la phrase avec le bow existant\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Transformer la nouvelle phrase en vecteur BoW en utilisant le vectoriseur, pour \"bowiser\" avec le bow existant\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m bow_phrase \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphrase_random_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# étape 2 : faire le prédict\u001b[39;00m\n\u001b[1;32m     12\u001b[0m langue_pred \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(bow_phrase)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1432\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1432\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1434\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1273\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Prédire la langue de la phrase sélectionnée au hasard\n",
    "\n",
    "    # étape 1 : vectoriser en bag of words la phrase sélectionnée\n",
    "phrase_random_df = pd.DataFrame({'Phrases': phrase_random},index=[0]) # transfo de la phrase 'string' en dataframe car ma fonction BOW fonctionne avec des dataframes\n",
    "\n",
    "#bow_phrase = BOW(phrase_random_df) ## hé non ! car cela créer un nouveau bow alors qu'il faut analyser la phrase avec le bow existant\n",
    "\n",
    "    # Transformer la nouvelle phrase en vecteur BoW en utilisant le vectoriseur, pour \"bowiser\" avec le bow existant\n",
    "bow_phrase = vectorizer.transform([phrase_random_df])\n",
    "\n",
    "    # étape 2 : faire le prédict\n",
    "langue_pred = knn.predict(bow_phrase)\n",
    "print(\"La phrase suivant :\\n\", phrase_randomlangue_pred, \"/n est en :\", langue_pred)\n",
    "\n",
    "\n",
    "## note pour moi : ça ne marche pas car je n'ai pas créer de vectorizer commun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créez une instance de SVM à noyau linéaire\n",
    "#svm = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# entrainement sur le train\n",
    "#svm.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "#svm.predict(X_test)\n",
    "\n",
    "# score\n",
    "#precision = svm.score(X_test, y_test)\n",
    "\n",
    "#print (precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7e0a991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## etapes d'après :\n",
    "# essayer avec des mots non contenus dans le bow et donc pouvoir saisir des phrases inventées non contenus dans les datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4dd12387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrez votre nom : Keyne\n",
      "Bonjour, Keyne !\n"
     ]
    }
   ],
   "source": [
    "# Test saisie de texte\n",
    "user_input = input(\"Entrez votre nom : \")\n",
    "print(\"Bonjour, \" + user_input + \" !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbed19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0eebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
